% Advanced Programming 2025 - Project Report Template
% HEC Lausanne / UNIL
\documentclass[11pt,a4paper]{article}

% Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[english]{babel}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{listings}
\usepackage{hyperref}
\usepackage[margin=1in]{geometry}
\usepackage{fancyhdr}
\usepackage{float}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{biblatex}
\addbibresource{references.bib}

% Code listing settings
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{pythonstyle}{
    backgroundcolor=\color{backcolour},
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,
    breaklines=true,
    captionpos=b,
    keepspaces=true,
    numbers=left,
    numbersep=5pt,
    showspaces=false,
    showstringspaces=false,
    showtabs=false,
    tabsize=2,
    language=Python
}
\lstset{style=pythonstyle}

% Header and footer
\pagestyle{fancy}
\fancyhf{}
\rhead{Advanced Programming 2025}
\lhead{Project Report}
\rfoot{Page \thepage}

% Title page information - MODIFY THESE
\title{%
    \Large \textbf{Advanced Programming 2025} \\
    \vspace{0.5cm}
    \LARGE \textbf{Predicting Football Match Outcomes Using Real Match Data} \\
    \vspace{0.3cm}
    \large Final Project Report
}
\author{
    Noah Grande \\
    \texttt{noah.grande@unil.ch} \\
    Student ID: 22407373
}
\date{\today}

\begin{document}

\maketitle
\thispagestyle{empty}

\begin{abstract}
Predicting the outcome of football matches is challenging. The various aspects of the game from team performance, tactical choices to diverse situations all play an important role in the result. This project investigates whether historical match statistics are meaningful to predict the outcome of football matches of the English Premier League.
Using real Premier League data, a dataset was constructed from publicly available match statistics and bookmaker odds from season 2021/22 to 2024/25, representing approximately 1400 matches. The data preparation work included data cleaning, merging datasets, feature selection and the creation of recent form rolling indicator, capturing differences between home and away teams. Two machine learning models, Logistic Regression and Random Forest classifier, were implemented to estimate outcome probabilities. The performance of the model was evaluated using standard classification metrics such as accuracy, log-loss and confusion matrix on unseen data. In addition, predictions of the model were compared to the implied probabilities of bookmakers, which serve as a strong baseline reflecting aggregated market information. The results show that the models achieve reasonable predictive performance and capture meaningful relationships between match statistics and result, while still exhibiting limitations compared to bookmaker probabilities.
This project outlines a full data science workflow, from data preparation to model evaluation, to examine if the outcome of a match can be predicted using historical match data, and how the probability estimates produced by machine learning models compare to those implied by bookmaker odds.
\end{abstract}

\vspace{0.5cm}
\noindent\textbf{Keywords:} data science, Python, machine learning, football analytics, match outcome prediction, feature engineering, supervised learning, probabilistic modeling

\newpage
\tableofcontents
\newpage

% ================== MAIN CONTENT ==================

\section{Introduction}
\label{sec:introduction}

Football is the most recognized and well-known sport in the world. The English Premier League is arguably one of the most entertaining and competitive leagues globally. Beyond its popularity, football also generates a massive amount of publicly available data, making it an attractive domain for computer science and machine learning applications focused on match analysis and prediction. Despite this abundance of information, forecasting match outcomes remains challenging. Results are influenced by many factors ranging from weather conditions and tactical decisions to in-game events and chance.

The main objective of this project is to investigate whether historical match statistics can serve as meaningful predictors of match results, defined as home win, draw, or away win. The goal is not to produce a betting system nor to outperform bookmakers; instead, the project aims to understand the relationship between measurable performance indicators and match outcomes. In this context, bookmakers' odds are not viewed as a target to beat, but as a sophisticated benchmark representing aggregated market expectations.

To address this problem, a dataset was constructed using real match statistics such as goals, passing metrics, disciplinary measures, and betting odds. The model features were designed to capture relative differences between home and away teams, a common approach in football prediction. The features include recent performance indicators, goal-related metrics, and advanced statistics, aggregated over rolling windows to prevent data leakage. Two supervised learning models, logistic regression and random forest classifiers, were used to estimate outcome probabilities based on these engineered features.

This report walks through the full pipeline, from feature engineering and model training to a critical examination of where statistical learning methods succeed and where they fall short.

\section{Literature Review / Related Work}
\label{sec:literature}

Predicting football match outcomes has long been a subject of interest in both statistics and data science, and has evolved substantially with the increasing availability of detailed match data. In most studies, the task is formulated as a three-class classification problem (home win, draw, away win). Football presents a particularly challenging prediction problem due to its inherent uncertainty, the high impact of random events, and the strong interdependence between competing teams. Therefore, even with well-designed models, perfect prediction is practically unattainable.

Early research relied primarily on statistical models such as Poisson regression and goal-scoring processes. These approaches provided important foundations and introduced key assumptions about team strengths and match dynamics, but they often struggled to capture complex relationships between teams and contextual factors \cite{1,2}. More recent work has shifted toward machine learning models including logistic regression, random forests, and gradient boosting methods \cite{3,4,8,9}, which are better suited to handle nonlinear patterns and the large volume of heterogeneous football data.

A widely reported challenge is the prediction of draws. Several studies observe that models may perform reasonably well for wins and losses while predicting draws poorly or ignoring them altogether \cite{6}. This issue is often attributed to class imbalance, as draws occur less frequently than decisive outcomes. To mitigate this, some authors reformulate the task as binary classification (e.g., home win vs.\ non-home win), which can improve overall predictive performance \cite{5}.

Feature engineering plays a central role in football match prediction. Many studies emphasize the importance of using only features available prior to kick-off to ensure realistic predictive scenarios \cite{3,19}. Common features include aggregated statistics from recent matches (shots, goals, possession, disciplinary records), typically computed as rolling averages. Advanced indicators such as expected goals (xG) have also gained popularity, as they can provide more stable measures of performance than goals alone \cite{3,8}. Another common strategy is the use of relative features (differences in form and strength) between home and away teams. By focusing on differences rather than absolute levels, models encode competitive balance and home advantage effects, which are known to influence outcomes \cite{7,8,11}. External ratings (e.g., Elo) are also used as proxies for team strength and long-term performance \cite{11}.

Finally, bookmakers' odds are frequently used as a benchmark for evaluating predictive models. Since betting markets aggregate large amounts of information, bookmaker-implied probabilities are difficult to outperform \cite{12,13}. Rather than focusing on profitability, several studies use odds as a baseline to assess whether models capture additional signal beyond market expectations \cite{13,14}.

Overall, the literature suggests that machine learning models can achieve reasonable performance with careful feature engineering and problem formulation \cite{3,5,10}. However, limitations related to draw prediction, class imbalance, and generalization remain significant. This project builds on prior work by adopting a structured machine learning workflow, emphasizing relative performance features, probabilistic evaluation, and comparison to bookmaker-based baselines within an academic data science framework \cite{6,7,18}.

\section{Methodology}
\label{sec:methodology}

\subsection{Data Description}

The datasets used include both raw and processed data covering Premier League seasons from 2021 up to January 2025. The data collection period ends in January 2025 due to missing values in the records. The choice to focus on seasons from 2021 onward is motivated by the structural and tactical evolution of the Premier League in recent years. Restricting the analysis to more recent seasons increases relevance and consistency with the current football context.

The first raw dataset consists of historical Premier League match data collected from publicly available sources and Kaggle (\texttt{matchdata\_21-25.csv}). The data span multiple seasons (2021/22 to 2024/25 up to January) and is structured at the match level from one-team point of view, resulting in 1,369 games (2,738 rows). The dataset contains 152 columns capturing match statistics such as goal metrics, passing statistics, and advanced indicators such as xG.

In addition, four season-level datasets (\texttt{21\_22}, \texttt{22\_23}, \texttt{23\_24}, \texttt{24\_25}) were obtained from \texttt{football-data.co.uk}, containing match statistics combined with bookmaker odds. Each row corresponds to a match and includes both home and away information. These datasets were merged into \texttt{all\_matches.csv}, and a \texttt{match\_id} was created to facilitate merging. After cleaning, only relevant match statistics and selected odds were retained.

Both processed datasets were merged using \texttt{match\_id}. Rolling features were engineered to ensure the model uses only information available prior to each match. After preprocessing, the intermediate dataset (\texttt{data\_after\_engineering.csv}) contains 1,369 matches (two rows per match in intermediate team-view format) and a structured set of numerical rolling features such as \texttt{avg\_goals\_for\_L5}, \texttt{avg\_xg\_against\_L5}, and \texttt{avg\_discipline\_L5}. Rolling features resulted in the loss of approximately 80 matches due to missing values in early-season games. Rolling computations were restarted at the beginning of each season; for the first four games, averages were computed using the available previous matches.

The target variable (\texttt{target}) is a categorical match outcome with three classes: home win, draw, and away win, corresponding to the standard multiclass setup in football prediction research \cite{3,5}.

The final modeling dataset (\texttt{model\_data.csv}) contains one row per match and includes only variables available prior to kick-off. Most features are computed as differences between home and away teams to capture relative strength. These include differences in recent form (\texttt{diff\_avg\_points\_L5}, \texttt{diff\_avg\_points\_L10}), offensive and defensive performance (\texttt{diff\_avg\_goals\_for\_L5}, \texttt{diff\_avg\_goals\_against\_L5}), clean sheet rates, expected goals metrics, and other match statistics.

Crucially, the dataset also includes bookmaker odds available before kick-off: three-way odds (\texttt{odds\_win}, \texttt{odds\_draw}, \texttt{odds\_lose}) and over/under odds (\texttt{odds\_over25}, \texttt{odds\_under25}). These variables are used as a benchmark baseline and are not included as model inputs \cite{12,13}.

\subsection{Approach}

This project adopts a supervised machine learning approach to predict football match outcomes. The task is formulated as a multiclass classification problem with three possible outcomes: home win, draw, and away win. This formulation is standard in the football prediction literature and reflects the natural structure of match results \cite{3,5}.

\subsubsection{Algorithms}

Two classification models are employed: logistic regression and random forest. Logistic regression is used as a baseline due to its simplicity, interpretability, and ability to produce well-calibrated probabilistic outputs. Its linear structure offers a transparent reference point for comparison.

Random forest is selected as a representative ensemble method based on decision trees. By aggregating predictions from multiple trees, the model captures nonlinear relationships and interactions between features while remaining robust to noise and outliers \cite{8,9,15}. This is particularly suitable for heterogeneous football performance data.

Bookmaker odds are not included as input features in the machine learning models. Instead, they are used separately as an external benchmark, comparing model-predicted probabilities to bookmaker-implied probabilities \cite{12,13}.

\subsubsection{Preprocessing}

Observations with missing values are removed to ensure a consistent feature set. Rolling features are constructed to ensure models use only information available prior to kick-off. They are generated as follows:

\begin{lstlisting}[caption={Rolling feature construction example}]
df["avg_points_L5"] = g["points"].shift(1).rolling(5, min_periods=1).mean()
\end{lstlisting}

This example from the \texttt{build\_rolling\_features} function computes the mean of the previous five games while excluding the current match to avoid leakage. If fewer than five prior matches are available, the mean is computed using the available matches.

Difference-based features are then constructed from home and away rolling features:

\begin{lstlisting}[caption={Difference feature construction example}]
for c in feature_cols:
    out[f"diff_{c}"] = (merged[f"{c}_home"] - merged[f"{c}_away"])
\end{lstlisting}

These differences encode relative performance and are commonly used in football prediction studies.

For logistic regression, numerical features are standardized to zero mean and unit variance to improve numerical stability. For random forest, features are left unscaled, as tree-based models are invariant to monotonic transformations. No explicit outlier removal is performed. Feature selection is guided by domain knowledge and correlation-based considerations to reduce redundancy and limit overfitting \cite{18,19}.

\subsubsection{Model Architecture}

Logistic regression estimates class probabilities for home win, draw, and away win. The predicted class corresponds to the highest probability. Regularization is applied to reduce overfitting and improve generalization.

Random forest is an ensemble of decision trees trained on bootstrap samples. Each tree uses a random subset of features, increasing diversity. Predictions are formed by averaging class probabilities across trees.

Both models are implemented using \texttt{scikit-learn}, ensuring consistent training, probability estimation, and evaluation.

\subsubsection{Evaluation Metrics}

Performance is evaluated using both classification and probabilistic metrics. Accuracy measures the fraction of correct class predictions, but is insufficient due to outcome imbalance (draws occur less frequently). Therefore, log loss is also used to assess the quality of predicted probability distributions, penalizing confident incorrect predictions and enabling comparison with bookmaker probabilities \cite{6}.

Results are summarized in a comparative table including accuracy and log loss for each model and for the bookmaker baseline. Class-level performance is examined using confusion matrices and classification reports, with particular focus on the draw class, which is known to be difficult to predict \cite{5,10}.

Finally, model predictions and bookmaker probabilities are compared in \texttt{match\_probabilities\_comparison.csv} to highlight matches where the model assigns higher probability to the realized outcome than the bookmakers.

\subsection{Implementation}

The project is implemented entirely in Python using standard data science libraries. Data manipulation and preprocessing are handled with \texttt{pandas} and \texttt{numpy}. Machine learning models are implemented using \texttt{scikit-learn}. Model persistence is managed with \texttt{joblib}, ensuring trained models can be saved and reused.

The system architecture follows a modular and reproducible pipeline. Raw match data are loaded and sorted chronologically to prevent leakage. Features are selected and validated, and the dataset is split chronologically into training and test sets (80/20). Models are trained on the training set, evaluated on the test set, and stored together with evaluation outputs.

A key part of the project is data processing: loading, standardizing, merging, feature selection, and rolling feature creation. Logistic regression is implemented as a pipeline combining \texttt{StandardScaler} and model fitting. Random forest is trained without scaling and provides feature importance scores for interpretation. Classification reports, confusion matrices, and probabilistic metrics are generated and saved. Trained models and feature lists are stored for reproducibility.

\section{Results}
\label{sec:results}

\subsection{Experimental Setup}

All experiments were conducted in a CPU-only environment on a standard personal computer. The implementation was carried out in Python using \texttt{pandas} and \texttt{scikit-learn}. The full training and evaluation procedure is reproducible and executed locally.

To avoid temporal leakage, matches were sorted by \texttt{match\_date} and split using an 80/20 chronological split. Approximately 80\% of observations (1,027 matches) were used for training and 20\% (257 matches) for testing.

\begin{lstlisting}[caption={Chronological split}]
df = df.sort_values("match_date")
split_idx = int(len(x) * 0.8)
\end{lstlisting}

For logistic regression, a multinomial formulation was used with feature standardization via \texttt{StandardScaler}. Training relied on the LBFGS optimizer with increased iterations.

\begin{lstlisting}[caption={Logistic regression pipeline}]
log_reg = Pipeline([
    ("scaler", StandardScaler()),
    ("clf", LogisticRegression(
        solver="lbfgs",
        max_iter=2000,
        random_state=42
    ))
])
\end{lstlisting}

The random forest model used 300 trees with \texttt{min\_samples\_leaf=5} to reduce overfitting.

\begin{lstlisting}[caption={Random forest configuration}]
rf = RandomForestClassifier(
    n_estimators=300,
    min_samples_leaf=5,
    random_state=42,
    n_jobs=-1
)
\end{lstlisting}

No cross-validation or extensive hyperparameter tuning was performed, as the objective is comparative and methodological rather than performance optimization.

\subsection{Performance Evaluation}

Model performance was evaluated using both classification accuracy and probabilistic metrics. In addition to the machine learning models, bookmaker-implied probabilities were included as a baseline benchmark. For comparability, bookmaker predictions were derived by selecting the most likely outcome (argmax) from the implied probability distribution.

Table 1 presents the predictive performance of the two machine learning models and the bookmaker baseline on the test set, evaluated using both classification accuracy and log-loss to capture not only correctness but also the quality of predicted probabilities.

\begin{table}[H]
\centering
\caption{Predictive performance comparison on the test set}
\label{tab:performance}
\begin{tabular}{|l|c|c|}
\hline
\textbf{Model} & \textbf{Accuracy} & \textbf{Log-loss} \\
\hline
Logistic Regression & 0.5370 & 0.9988 \\
Random Forest & 0.5292 & 0.9984 \\
Bookmaker Baseline & 0.5474 & 0.9438 \\
\hline
\end{tabular}
\end{table}

All three approaches achieve similar performance, with results slightly above 53\% accuracy. The bookmaker baseline marginally outperforms both machine learning models in terms of accuracy and log-loss, confirming its role as a strong reference point.
Class-level evaluation reveals a systematic difficulty in predicting draw outcomes. Both machine learning models almost never predict the draw class correctly, a limitation that is also observed in the bookmaker baseline. In contrast, home wins and away wins are predicted with substantially higher recall, especially for home wins. These results highlight the inherent class imbalance of football match outcomes and confirm findings reported in prior literature regarding the difficulty of draw prediction.

\subsection{Visualizations}

To complement aggregate performance metrics, confusion matrices were used to analyze class-level prediction behavior for each approach. These visualizations provide insight into how models distribute their errors across outcome classes and help identify systematic weaknesses that are not captured by accuracy alone.

\subsubsection{Confusion Matrices}

Figure 1 presents the confusion matrix of the bookmaker baseline on the test set. Although bookmaker probabilities are used as a benchmark rather than a trained model, converting the highest implied probability into a predicted class allows a direct comparison with machine learning approaches. The bookmaker baseline correctly predicts a substantial number of home and away wins but, similarly to the machine learning models, fails to correctly classify draw outcomes. Draws are consistently misclassified as either home or away wins, highlighting the inherent difficulty of this class.

\begin{figure}[H]
\centering
\includegraphics[width=0.65\textwidth]{results/visualisation/confusion_matrix_bookmaker.png}
\caption{Confusion matrix of the bookmaker baseline on the test set (class order: Away win, Draw, Home win).}
\label{fig:cm_bookmaker}
\end{figure}

Figure 2 shows the confusion matrix of the logistic regression model. The model performs reasonably well for home wins and away wins, with a strong bias toward predicting decisive outcomes. However, no draw is correctly predicted. All draw matches are absorbed into the home win or away win categories. This behavior reflects both class imbalance and the limitations of linear decision boundaries when modeling marginal match situations.

\begin{figure}[H]
\centering
\includegraphics[width=0.65\textwidth]{results/visualisation/confusion_matrix_logistic_regression.png}
\caption{Confusion matrix of the logistic regression model on the test set (class order: Away win, Draw, Home win).}
\label{fig:cm_logreg}
\end{figure}

Figure 3 presents the confusion matrix of the random forest model. Despite its ability to capture non-linear relationships and interactions between features, the random forest exhibits a very similar error structure to logistic regression. Home wins and away wins are predicted with comparable accuracy, while draw outcomes are never correctly classified. This result suggests that the difficulty in predicting draws is structural and not resolved by increased model complexity.

\begin{figure}[H]
\centering
\includegraphics[width=0.65\textwidth]{results/visualisation/confusion_matrix_random_forest.png}
\caption{Confusion matrix of the random forest model on the test set (class order: Away win, Draw, Home win).}
\label{fig:cm_rf}
\end{figure}

Across all three approaches, misclassifications predominantly occur between home wins and away wins, while draw outcomes are systematically misclassified as one of the two win categories. This pattern is consistent with prior findings in football match prediction research and reflects the marginal nature of draws, which often arise from finely balanced matches and stochastic in-game events that are difficult to capture using aggregated pre-match statistics.

\subsubsection{Probabilistic Interpretation}

Beyond classification behavior, model predictions were also analyzed from a probabilistic perspective. In a match-level comparison, the logistic regression model assigned a higher probability to the realized outcome than the bookmaker baseline in 99 out of 257 matches (38.\%). While this does not translate into superior overall performance, it indicates that the model captures complementary information in a non-negligible subset of matches.

Importantly, this analysis is not intended to demonstrate systematic superiority over bookmakers, but rather to illustrate how probabilistic machine learning models may express uncertainty differently from market-based forecasts. In particular, model predictions tend to distribute probability mass more evenly across outcomes in closely balanced matches, whereas bookmaker probabilities are often more concentrated on a single result.

\section{Discussion}
\label{sec:discussion}

The results of this project provide several insights into the effectiveness and limitations of machine learning approaches for football match outcome prediction. Overall, the proposed methodology demonstrates that a structured and well-designed pipeline can extract meaningful information from historical match data, even in a domain characterized by high uncertainty.

One aspect that worked particularly well is the overall modelling framework. The use of relative, difference-based features capturing recent team performance proved effective in distinguishing home wins from away wins. Both logistic regression and random forest models produced stable probabilistic predictions and achieved performance levels comparable to the bookmaker baseline. In this context, matching bookmaker accuracy should be considered a positive result, as bookmaker odds aggregate expert knowledge and market information and are widely recognized as a strong benchmark.

Several challenges were encountered during the project, the most prominent being the difficulty of predicting draw outcomes. Both machine learning models almost entirely failed to predict draws correctly, a limitation that was also observed in the bookmaker baseline. This issue is primarily due to class imbalance and the fact that draws often arise from marginal or random match events that are difficult to capture using aggregated team-level statistics. Rather than attempting to artificially rebalance classes or modify the prediction task, this limitation was addressed through careful interpretation of the results and class-level performance analysis.

The observed results are largely consistent with the initial expectations of the project. It was hypothesized that machine learning models based on publicly available, team-level features would achieve moderate predictive performance but would struggle to consistently outperform bookmakers. The results confirm this hypothesis, as both models perform slightly below or at a similar level to the bookmaker baseline in terms of accuracy and log loss. Likewise, the persistent difficulty in draw prediction aligns with findings commonly reported in the literature.

Despite the use of a more complex model, random forest did not significantly outperform logistic regression. This highlights an important limitation of the current approach: increasing model complexity does not necessarily lead to better performance when the feature set is limited. The models rely exclusively on aggregated historical statistics and do not account for important contextual factors such as player availability, injuries, tactical decisions, or in-game dynamics. In addition, no advanced calibration techniques or extensive hyperparameter tuning were applied, as the primary focus of the project was methodological clarity rather than performance optimization.

One somewhat surprising finding is that, although the models do not outperform the bookmaker baseline overall, the logistic regression model assigns a higher probability to the realized outcome than the bookmaker in approximately 38.5\% of matches. This suggests that the model captures complementary information and occasionally disagrees with the market in a meaningful way, even if this is insufficient to achieve consistent superiority. This result highlights the potential value of probabilistic analysis beyond simple accuracy comparisons.

In summary, the discussion of the results reinforces the idea that football match prediction is inherently difficult and that moderate performance levels should be expected when relying on historical, aggregated data. The project successfully demonstrates a sound data science workflow, provides realistic insights into the strengths and weaknesses of machine learning models in this domain, and aligns well with both the course objectives and existing research.

\section{Conclusion and Future Work}
\label{sec:conclusion}

\subsection{Summary}

This project investigated the application of machine learning techniques to football match outcome prediction using historical data from the English Premier League. A complete and reproducible data science pipeline was developed, covering data preparation, feature engineering, model training, evaluation, and comparison with bookmaker-based baselines.

The main achievement of the project is the successful implementation of a structured predictive framework using logistic regression and random forest models. Both models achieved performance levels comparable to bookmaker predictions, demonstrating that relative, team-level performance features capture meaningful information about match outcomes. The project also highlighted the importance of probabilistic evaluation, showing that model outputs can be meaningfully compared to bookmaker-implied probabilities.

The project objectives were fully met. Rather than attempting to outperform betting markets, the work focused on methodological clarity, realistic evaluation, and critical interpretation of results. The findings contribute to a better understanding of the strengths and limitations of machine learning approaches in football analytics and align well with established results in the literature.

\subsection{Future Directions}

Several directions could be explored to extend and improve this work. From a methodological perspective, future research could investigate alternative modeling approaches, such as calibrated ensemble methods or hierarchical models, to better handle class imbalance and improve draw prediction. Incorporating probability calibration techniques may also enhance the quality of predicted probabilities.

Additional experiments could include expanding the feature set to incorporate player-level information, injuries, squad rotation, or tactical variables, which are known to influence match outcomes. Evaluating the models across multiple leagues or longer time periods could also provide insights into their generalizability and robustness.

From an applied perspective, the framework developed in this project could be adapted for real-world use in match analysis or decision-support systems, where probabilistic forecasts are more valuable than deterministic predictions. Finally, scalability considerations could involve automating data updates and extending the pipeline to handle larger datasets or real-time prediction scenarios.

Finally, while this study focuses exclusively on the English Premier League, the proposed methodology is not league-specific and could be generalized to a broader set of competitions. Applying the same pipeline to multiple leagues would allow for comparative analyses across different football contexts, styles of play, and competitive structures. Such an extension could help assess the robustness and generalizability of the modeling approach, as well as identify league-dependent patterns in match outcome prediction. This multi-league perspective would further strengthen the validity of the findings and provide a more comprehensive understanding of the applicability of machine learning methods in football analytics.

% ================== REFERENCES ==================
\newpage
\begin{thebibliography}{99}

\bibitem{ref1}
Dixon, M. J., \& Coles, S. G. (1997).
\textit{Modelling association football scores and inefficiencies in the football betting market}.
Journal of the Royal Statistical Society: Series C (Applied Statistics), 46(2), 265--280.

\bibitem{ref2}
Maher, M. J. (1982).
\textit{Modelling association football scores}.
Statistica Neerlandica, 36(3), 109--118.

\bibitem{ref3}
Baboota, R., \& Kaur, H. (2019).
\textit{Predictive analysis and modelling football results using machine learning}.
International Journal of Forecasting, 35(2), 741--755.

\bibitem{ref4}
Bunker, R. P., \& Thabtah, F. (2019).
\textit{A machine learning framework for sport result prediction}.
Applied Computing and Informatics, 15(1), 27--33.

\bibitem{ref5}
Bunker, R. P., \& Susnjak, T. (2022).
\textit{Machine learning techniques for predicting match results in team sport: A review}.
IEEE Access, 10, 25193--25212.

\bibitem{ref6}
Constantinou, A. C., \& Fenton, N. E. (2012).
\textit{Solving the problem of inadequate scoring rules for probabilistic football forecasts}.
Journal of Quantitative Analysis in Sports, 8(2).

\bibitem{ref7}
Constantinou, A. C., \& Fenton, N. E. (2013).
\textit{Determining team ability using dynamic ratings}.
Journal of Quantitative Analysis in Sports, 9(4), 379--392.

\bibitem{ref8}
Groll, A., Schauberger, G., \& Tutz, G. (2018).
\textit{Prediction of the FIFA World Cup 2018 -- a random forest approach}.
Journal of Quantitative Analysis in Sports, 15(3), 1--15.

\bibitem{ref9}
Groll, A., Schauberger, G., \& Tutz, G. (2019).
\textit{A hybrid random forest to predict soccer matches}.
Statistical Modelling, 19(2), 123--146.

\bibitem{ref10}
Tsokos, A., et al. (2019).
\textit{Modeling outcomes of soccer matches}.
Machine Learning, 108(1), 1--24.

\bibitem{ref11}
Hvattum, L. M., \& Arntzen, H. (2010).
\textit{Using Elo ratings for match result prediction in association football}.
International Journal of Forecasting, 26(3), 460--470.

\bibitem{ref12}
Forrest, D., Goddard, J., \& Simmons, R. (2005).
\textit{Odds-setters as forecasters: The case of English football}.
International Journal of Forecasting, 21(3), 551--564.

\bibitem{ref13}
\textit{\v{S}trumbelj}, E., \& \textit{\v{S}ikonja}, M. R. (2010).
\textit{Online bookmakers' odds as forecasts: The case of European soccer leagues}.
International Journal of Forecasting, 26(3), 482--488.

\bibitem{ref14}
Wheatcroft, E. (2020).
\textit{A profitable model for predicting the over/under market in football}.
Journal of Sports Analytics, 6(2), 89--103.

\bibitem{ref15}
Breiman, L. (2001).
\textit{Random forests}.
Machine Learning, 45(1), 5--32.

\bibitem{ref16}
Chen, T., \& Guestrin, C. (2016).
\textit{XGBoost: A scalable tree boosting system}.
Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 785--794.

\bibitem{ref17}
Ke, G., et al. (2017).
\textit{LightGBM: A highly efficient gradient boosting decision tree}.
Advances in Neural Information Processing Systems (NeurIPS), 30, 3146--3154.

\bibitem{ref18}
Hall, M. A. (1999).
\textit{Correlation-based feature selection for machine learning}.
PhD Thesis, University of Waikato.

\bibitem{ref19}
Zheng, A., \& Casari, A. (2018).
\textit{Feature engineering for machine learning: Principles and techniques for data scientists}.
O'Reilly Media.

\bibitem{ref20}
Lundberg, S. M., \& Lee, S. I. (2017).
\textit{A unified approach to interpreting model predictions}.
Advances in Neural Information Processing Systems (NeurIPS), 30, 4765--4774.

\bibitem{ref21}
Van Wijk, D. (2020).
\textit{Machine learning approaches for football match outcome prediction}.
Master's Thesis, Erasmus University Rotterdam.

\bibitem{ref22}
Mendel Journal. (2017).
\textit{Machine learning approaches in football analytics}.
Available at: \url{https://mendel-journal.org/index.php/mendel/article/view/263/217}

\bibitem{ref23}
First Data set
\textit{Match_data_21_25}.
Available at: \url{}

\end{thebibliography}


% ================== APPENDICES ==================
\newpage
\appendix

\section{Additional Results}
\label{app:figures}

This appendix presents supplementary results supporting the main findings.

\subsection{Detailed Classification Reports}

% If you want to include text files directly, use \lstinputlisting (recommended).
\lstinputlisting[language={},caption={Logistic regression classification report}]{results/logistic_regression_report.txt}
\lstinputlisting[language={},caption={Random forest classification report}]{results/random_forest_report.txt}
\lstinputlisting[language={},caption={Bookmaker baseline classification report}]{results/bookmaker_baseline_report.txt}

\subsection{Model Interpretability: Coefficients and Feature Importance}

\lstinputlisting[language={},caption={Logistic regression coefficients}]{results/logistic_regression_coefficients.txt}
\lstinputlisting[language={},caption={Random forest feature importances}]{results/random_forest_feature_importance.txt}

\section{Code Repository}
\label{app:code}

The complete source code for this project is available in a public GitHub repository. 
The repository contains all scripts used for data preprocessing, feature engineering, 
model training, evaluation, and result generation.

\noindent
\textbf{GitHub Repository:} \url{https://github.com/noahgrande/epl-outcome-prediction.git}

\subsection{Repository Structure}

The repository follows a clear and modular structure to ensure readability and reproducibility:

\begin{verbatim}
epl-outcome-prediction/
├── data/
│ ├── raw/
│ └── processed/
├── models/
│ ├── logistic_regression.pkl
│ ├── random_forest.pkl
│ └── features_list.txt
├── results/
│ ├── bookmakers_baseline_report.txt
│ ├── final_results_summary.txt
│ ├── logistic_regression_coefficients.txt
│ ├── logistic_regression_report.txt
│ ├── logistics_regression_summary.txt
│ ├── match_probabilities_comparison.csv
│ ├── random_forest_feature_importance.txt
│ └── random_forest_report.txt 
├── src/
│ ├── __init__.py
│ ├── data_loader.py
│ ├── models.py
│ ├── probabilistic_evaluation.py
│ └── statistics_analysis.py
├── tests/
│ └── test_pipeline.py
├── .gitignore
├── environment.yml
├── main.py
├── project_report.tex
├── PROPOSAL.md
└── README.md
\end{verbatim}

This structure separates raw data, processed datasets, source code, results, and trained models, 
following good software engineering practices.

\subsection{Installation Instructions}

The project is implemented in Python and relies on standard data science libraries. 
The recommended way to recreate the environment is via Conda using the provided 
\texttt{environment.yml} file.

\begin{verbatim}
git clone https://github.com/noahgrande/epl-outcome-prediction.git
cd epl-outcome-prediction
conda env create -f environment.yml
conda activate epl-prediction
\end{verbatim}

Alternatively, the dependencies can be installed manually using \texttt{pip} if required.

\subsection{Reproducing the Results}

All experiments and evaluations reported in this study can be reproduced by running the main 
execution script. The pipeline performs data loading, feature engineering, model training, 
evaluation, and comparison with the bookmaker baseline.

\begin{verbatim}
python src/main.py
\end{verbatim}

The script generates trained models, evaluation reports, confusion matrices, and probabilistic 
comparison files in the \texttt{results/} directory. The chronological train/test split ensures 
that no temporal leakage occurs during evaluation.


\section{AI Usage Statement}

\subsection{Use of Artificial Intelligence Tools}

This project was developed with limited and transparent use of Artificial Intelligence (AI) tools, in accordance with the course rules and academic integrity guidelines.

AI tools (notably large language models such as ChatGPT) were used \textbf{exclusively as supportive tools} during the development process. 

Their use was restricted to the following purposes:

- Clarifying programming concepts (e.g. Python syntax, machine learning workflows, evaluation metrics).

- Improving code readability, structure, and documentation (comments, docstrings, README formatting).

- Assisting with debugging by suggesting potential causes of errors or alternative implementations.

- Helping reformulate explanations in a clearer and more academic style, particularly in English.

\subsection{What AI Was Not Used For}

AI tools were not used for:

- Generating final model results or empirical findings.

- Designing the core methodology of the project.

- Making modeling decisions without human validation.

- Automatically writing complete solutions that were submitted without understanding or modification.

- Producing figures, tables, or numerical outputs directly used in the report.

All modeling choices, data preprocessing steps, feature engineering decisions, experiments, evaluations, and interpretations were **designed and implemented by student**.

\end{document}
