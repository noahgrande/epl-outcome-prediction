{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# EPL Match Outcome Prediction\n",
        "## Notebook 02 — Model Training & Evaluation (ML vs Bookmaker)\n",
        "\n",
        "This notebook focuses on the **core supervised-learning step** of the project:\n",
        "\n",
        "- Train two models on engineered features:\n",
        "  - **Multinomial Logistic Regression** (interpretable linear baseline)\n",
        "  - **Random Forest** (non-linear baseline)\n",
        "- Evaluate both on a **time-based holdout test set**\n",
        "- Compare to a **bookmaker implied-probability baseline** (no training)\n",
        "\n",
        "### Why this notebook exists (academic motivation)\n",
        "Your pipeline builds a chronological dataset of Premier League matches.  \n",
        "To avoid **data leakage** (training on “future” information), we evaluate with a **temporal split**:\n",
        "\n",
        "- **Train:** first 80% of matches (older)\n",
        "- **Test:** last 20% of matches (more recent)\n",
        "\n",
        "This reflects the real use-case: predicting upcoming matches using only past data.\n",
        "\n",
        "---\n",
        "\n",
        "**Expected input file**\n",
        "- `../data/processed/model_data.csv`\n",
        "\n",
        "**Outputs produced by your code (`src/models.py`)**\n",
        "- `results/*_report.txt` (classification reports + log-loss)\n",
        "- `results/visualisation/*.png` (confusion matrices)\n",
        "- `results/logistic_regression_coefficients.txt`\n",
        "- `results/random_forest_feature_importance.txt`\n",
        "- `models/logistic_regression.pkl`, `models/random_forest.pkl`\n",
        "- `models/feature_list.txt`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 0) Imports and setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'Python 3.12.3' requires the ipykernel package.\n",
            "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Create a Python Environment</a> with the required packages."
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "\n",
        "# Project functions (these implement your official evaluation + artifact saving)\n",
        "from src.models import evaluate_bookmaker, train_models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ensure output folders exist (your functions also create them, but we keep it explicit)\n",
        "Path(\"../results\").mkdir(parents=True, exist_ok=True)\n",
        "Path(\"../results/visualisation\").mkdir(parents=True, exist_ok=True)\n",
        "Path(\"../models\").mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(\"✅ Output folders ready.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1) Load the final ML dataset (`model_data.csv`)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "DATA_PATH = Path(\"../data/processed/model_data.csv\")\n",
        "if not DATA_PATH.exists():\n",
        "    raise FileNotFoundError(\n",
        "        f\"Cannot find {DATA_PATH}. \"\n",
        "        \"Run your pipeline (main.py / notebook 03) to generate it, \"\n",
        "        \"or update DATA_PATH to the correct location.\"\n",
        "    )\n",
        "\n",
        "df_model = pd.read_csv(DATA_PATH)\n",
        "df_model[\"match_date\"] = pd.to_datetime(df_model[\"match_date\"], errors=\"coerce\")\n",
        "df_model = df_model.sort_values(\"match_date\").reset_index(drop=True)\n",
        "\n",
        "print(\"Shape:\", df_model.shape)\n",
        "print(\"Date range:\", df_model[\"match_date\"].min(), \"→\", df_model[\"match_date\"].max())\n",
        "df_model.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2) Sanity checks (what we verify and why)\n",
        "\n",
        "We verify that:\n",
        "- `target` exists and follows your encoding: **Home win = 1, Draw = 0, Away win = -1**\n",
        "- feature columns start with `diff_...` (home minus away rolling averages)\n",
        "\n",
        "These checks help ensure we are training on the intended design:\n",
        "> **match-level differences** that summarize recent team performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check target\n",
        "if \"target\" not in df_model.columns:\n",
        "    raise ValueError(\"Missing required column: target\")\n",
        "\n",
        "print(\"Target distribution (proportion):\")\n",
        "print(df_model[\"target\"].value_counts(normalize=True).sort_index())\n",
        "\n",
        "# Check diff_ features\n",
        "diff_cols = [c for c in df_model.columns if c.startswith(\"diff_\")]\n",
        "print(\"\\nNumber of engineered diff_ features:\", len(diff_cols))\n",
        "print(\"Example features:\", diff_cols[:8])\n",
        "\n",
        "if len(diff_cols) == 0:\n",
        "    raise ValueError(\"No diff_ features found. Expected columns starting with 'diff_'.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3) Define the test set (time-based split)\n",
        "\n",
        "**Why time-based split?**  \n",
        "Because in football prediction we must not learn from matches that occur after the ones we want to predict.\n",
        "\n",
        "So we split chronologically:\n",
        "- first 80% → training (past)\n",
        "- last 20% → test (future relative to train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Drop rows with missing values (your main.py does this before training)\n",
        "df_model = df_model.dropna().reset_index(drop=True)\n",
        "\n",
        "split_idx = int(len(df_model) * 0.8)\n",
        "df_train = df_model.iloc[:split_idx].reset_index(drop=True)\n",
        "df_test  = df_model.iloc[split_idx:].reset_index(drop=True)\n",
        "\n",
        "print(f\"Train size: {len(df_train)} | Test size: {len(df_test)}\")\n",
        "print(\"Train date range:\", df_train[\"match_date\"].min(), \"→\", df_train[\"match_date\"].max())\n",
        "print(\"Test  date range:\", df_test[\"match_date\"].min(), \"→\", df_test[\"match_date\"].max())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4) Bookmaker baseline (no model training)\n",
        "\n",
        "This baseline converts **decimal odds** into **implied probabilities**:\n",
        "- \\( p_i = 1 / odds_i \\)\n",
        "- then probabilities are normalized to account for overround.\n",
        "\n",
        "We then predict the class with the highest implied probability and evaluate:\n",
        "- **Accuracy**\n",
        "- **Log loss** (probabilistic quality)\n",
        "- **Confusion matrix** (saved as PNG)\n",
        "\n",
        "This gives a strong, realistic benchmark."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "book_metrics = evaluate_bookmaker(df_test)\n",
        "book_metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5) Train ML models and evaluate on the holdout test set\n",
        "\n",
        "We train:\n",
        "- **Logistic Regression** with standardization  \n",
        "  Why? Interpretable coefficients and a strong linear baseline.\n",
        "- **Random Forest**  \n",
        "  Why? Captures non-linear effects and interactions without heavy preprocessing.\n",
        "\n",
        "Your `train_models()` function:\n",
        "- uses all `diff_...` features\n",
        "- applies the same chronological split internally (80/20)\n",
        "- saves:\n",
        "  - reports (`.txt`)\n",
        "  - confusion matrices (`.png`)\n",
        "  - model files (`.pkl`)\n",
        "  - feature list used for training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "log_model, rf_model, metrics = train_models(df_model, book_metrics)\n",
        "metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6) Where to find the outputs\n",
        "\n",
        "After running the notebook, check:\n",
        "\n",
        "**Models**\n",
        "- `../models/logistic_regression.pkl`\n",
        "- `../models/random_forest.pkl`\n",
        "- `../models/feature_list.txt`\n",
        "\n",
        "**Reports**\n",
        "- `../results/bookmaker_baseline_report.txt`\n",
        "- `../results/logistic_regression_report.txt`\n",
        "- `../results/random_forest_report.txt`\n",
        "- `../results/final_results_summary.txt`\n",
        "\n",
        "**Interpretability**\n",
        "- `../results/logistic_regression_coefficients.txt`\n",
        "- `../results/random_forest_feature_importance.txt`\n",
        "\n",
        "**Figures**\n",
        "- `../results/visualisation/confusion_matrix_bookmaker.png`\n",
        "- `../results/visualisation/confusion_matrix_logistic_regression.png`\n",
        "- `../results/visualisation/confusion_matrix_random_forest.png`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7) Notes (optional improvements)\n",
        "\n",
        "If you want more robust evaluation for the dissertation/report:\n",
        "- **Walk-forward validation** (rolling-origin evaluation)\n",
        "- Probability calibration (isotonic / Platt scaling)\n",
        "- Confidence-based analysis: performance vs predicted probability bins"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
